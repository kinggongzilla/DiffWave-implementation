RAVE CHANGES MADE:


1. install local rave version (github clone) with “pip install -e .” —> to do that i first had to manually set version to “v2” in setup.py line 6

2. Add “scripts/encode.py” and “scripts/decode.py” files

3. add “”encode” and “decode” to main_cli.py AVAILABLE_SCRIPTS and in main() function (if elif statements)


5. set transform_to_spectrogram params to config from diffwave

6. add AudioFilesDataset to load audio from file system


DIFFWAVE CHANGES MADE:

1. changed waveform_in input channels to 128 (latent size of RAVE)

2. changed spectrogram conditioner to reduce size with convolutions instead of transpose conv2d





—————
I created LatentDataset in RAVE/rave/dataset.py to load latents created by DiffWave and then feed to decoder script.
———————————




6. August 2023
produced samples by sampling from diffwave into latent space and then putting through decoder are silent. 

7. August
I compared samples produced by the encoder and samples produced by diffwave. Both have mean around zero, however encoder samples have max and min around +/- 4 and diffwave samples have max and min exactly at +/- 1

I tried simply multiplying the magnitude of generated latent representations by diffwave by 4 and then feeding to decoder. However this did not produce samples with larger magnitude.



—> does diffwave clip to 1

—> flatten latent representations

—> Mini CNN modell anstatt DiffWave modell um zu sehen ob damit der Loss runter geht (ggf ohne conditioning für einfachheit)


—> Bei RAVE ausprobieren, ob ich zwischen zwei Encoded audio samples interpolieren kann und aus dem Decoder noch immer etwas sinnvolles rauskommt. —> Latent space könnte überparametrisiert sein. 




————————

Experiments:

Low LR: 2*1e-6 —> loss 0.8
standard LR: 2*1e-4 —> loss 0.8
High LR: 2*1e-2 —> loss 0.8

Large Batches: 48 —> loss 0.8
standard batches: 24 —> loss 0.8
small batches: 6 —> loss 0.8

many blocks: 60 —> loss 0.8
standard blocks: 30 —> loss 0.8
few blocks: 10 —> loss 0.8

many res channles: 256 —> loss 0.8
standard res channels: 64 —> loss 0.8

many timesteps: 500 —> loss 0.8
standard time steps: 50 —> 0.8

with conditioner: True —> loss 0.8
with conditioner: False -> loss 0.8

simple CNN with 3 2dConv layers stays at same loss of 0.8

Remove sqrt in this calculation: —> loss 0.8     
latent = torch.sqrt(alpha_cum[t])*latent + torch.sqrt(1-alpha_cum[t])*noise 

MSE Loss (both models) —> loss 1.0 

floating point precision: 32 —> 0.8
floating point precision: 16 —> 0.8

flattening input (latent space variables): —> loss ~0.65

flattened input + using 500 diffusion steps: —> loss ~0.25


flattened input + using 1000 diffusion steps: —> loss ~0.21

Increasing number of diff steps from 1000 -> 2000 reduces loss on EPOCH 1 from 0.37 to 0.24

increasing the number of channels in spectrogram conditioner makes loss function for first few epochs smoother —> but have to train for longer to get definitive result

Doubling learning rate increased loss

Halving batch size to 36 and doubling spec conditioner channels to 128 —> this seems to reduce the loss curve. However it still seems a bit choppy, already increasing loss after 3 epochs

Reducing learning rate from 2 * 1e-4 to 6 * 1e-5 —> loss is basically the same pattern

Setting learning rate to 2*1e-4, reducing batch size to 18; increasind residual channels to 128 —> loss is lower

adding 2 more conv transpose layers to spec conditioner —> loss is worse

removing the two extra conv layers and reducing channels from 128 to 1 —> loss approx the same as with two conv layers and 128; but training is faster and less memory

Taking mean of multiple Rave encodings results in something sounding like a bad mashup of three songs. 

Increasing num blocks from 30 to 50, keeping res channels at 128 (up from 64) —> 11M params —> loss is higher for first 2 epochs

30 blocks; 256 res channels 21M params —> loss only slightly lower 

26.August:
when comparing samples generated by diffwave sampling and RAVE audio encodings, The generated samples have very high mins, max, very high variance, means that are note zero. The encoded audios have min/max around 4 usually, and mean around zero
—> AudioLDM noise schedule leads to smaller min max values etc
—> DiffWave noise schedule leads to wayy higher min max values etc

changing noise schedule to: 1e-5, 0.0085 seems to improve the audio quality of a generated sample. However it generates the same audio regardless of the spectrogram conditioner

—> conditioning_variable magnitude is way smaller than the sample —> i think thats why it does not have any influence

—> The problem is that the magnitude of input x for DiffWave.forward() is much larger than the magnitude of the conditioner and also of the timestep embedding t.

Experiment: Scale conditioning var by constant 5 and normalize: (y + self.conv_conditioner(conditioning_var)*5)/2 and train model with this —> two samples are identical, so no influence from different conditioning vars noticed. 

Note: encoded audio samples follow (almost) a perfect normal distribution. 


Experiment: I added batch normalization: Decoded generated diffwave output is silent. Noise schedule used: 1e-4, 0.0085
Result: loss does not go down if batchnorm is added before each activation function in all blocks


Je tiefer die DiffWave block layer ist umso:
* x wird kleiner (layer 0: 3000)
* y wird kleiner (layer 0: 6000)
* conv_conditioner bleibt gleich bei 1000
* t bleibt ca gleich bei 140

Experiment: i tried scaling the various tensors, x, y, t and conv_conditioner output to similar magnitude as in waveform diffwave, (only in very first DiffWave Block)
Result: Scaling magnitude in first layer does not have any effect.
Moreover If i scale it in every diffwaveblock layer, loss does not go down


Experiment: use a batchnorm layer for the y output of each diffwaveblock
—> brings improvement! Also tried batchnorming skip too but didnt change anything compared to only batchnorming y

When looking at magnitudes during sampling:
In original diffwave: x_t starts at magnitude of around 220 (timestep 50) and ends at around 200
the model output magnitude is 

when training on variance schedule 1e-5, 0.005, loss is MUCH higher than on 0.0015, 0.0195

Effect on Loss of different variance schedules: see “comparing “variance schedule
 runs in wandb.

When using large variance schedules the loss goes much lower during training. However sampling seems unstable, with the norm exploding with each timestep.
—> I am currently experimenting with using PowerTransform on the inference model output.
—> 

idea/thought: when using batch size 64 there are only 141 update steps in one epoch. When using 1000 diffusion timesteps, the model only sees a fraction of diffusion timesteps in each epoch. Hence if randomly during some epoch some timesteps are sampled that it has not see or only seen fewer times, then the loss is higher
—> This should be tested!

When training with high variance schedule 1e-4,0.05 the loss is low. Moreover the norm of the model output is actually stable (when changing variance schedule after training, the sampling is unstable)
However output samples are silent


TODOS:

Wie groß sind skip connection magnitudes
wie groß ist model output/layer output
Wie affected der Block Index den magnitude

